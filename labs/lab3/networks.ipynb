{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Network Science\n",
    "\n",
    "**Complex Social Systems, 2025** </br>\n",
    "*Onur Akman, Lab 3, 26/03/2025*\n",
    "\n",
    "Colab: https://colab.research.google.com/drive/1MZzsrD5TsyRgb9toT-qhwTb0N0Ib_jt0?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Refresher: Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph([(1, 2), (2, 3), (3, 4)])\n",
    "print(f'\\\n",
    "    Edges: {G.edges}, \\n\\\n",
    "    Nodes: {G.nodes}, \\n\\\n",
    "    Degrees: {G.degree}, \\n\\\n",
    "    Connections (as numpy array):\\n{nx.to_numpy_array(G)} \\\n",
    "    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.circular_layout(G)\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "nx.draw_networkx(G, pos=layout)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Defining Random Networks\n",
    "\n",
    "$G_{n, p}$ Model: Each pair of `n` labeled nodes is connected with probability `p`. </br>\n",
    "In NetworkX, this basic family of random graphs is implemented as `fast_gnp_random_graph`. </br>\n",
    "Also known as an **Erdős-Rényi** graph or a **binomial graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.fast_gnp_random_graph(10, 0.3, seed=42)\n",
    "degree_sequence = sorted([d for _, d in G.degree()])\n",
    "degree_counter = Counter(degree_sequence)\n",
    "deg, cnt = zip(*degree_counter.items())\n",
    "\n",
    "print(f\"Average degree: {sum(degree_sequence) / len(degree_sequence)}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "plt.bar(deg, cnt)\n",
    "plt.title(\"Degree Histogram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1: Random graphs\n",
    "\n",
    ">In a binomial random graph, investigate how does an average **node degree** changes with increasing: <br>\n",
    ">    a) number of nodes; <br>\n",
    ">    b) increasing probability of linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2) Retrieve networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_df = pd.read_csv(\n",
    "    \"https://snap.stanford.edu/data/facebook_combined.txt.gz\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\" \",\n",
    "    names=[\"start_node\", \"end_node\"],\n",
    ")\n",
    "\n",
    "twitter_df = pd.read_csv(\n",
    "    \"https://snap.stanford.edu/data/twitter_combined.txt.gz\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\" \",\n",
    "    names=[\"start_node\", \"end_node\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Facebook data ---\\n\", facebook_df.sample(5, random_state=42))\n",
    "print(f\"Facebook data shape: {facebook_df.shape}\")\n",
    "print(\"\\n--- Twitter data ---\\n\", twitter_df.sample(5, random_state=42))\n",
    "print(f\"Twitter data shape: {twitter_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_G = nx.from_pandas_edgelist(facebook_df, \"start_node\", \"end_node\")\n",
    "print(\"-- Facebook Graph --\")\n",
    "print(f\"Graph type: {type(facebook_G)}\")\n",
    "print(f\"Number of nodes: {facebook_G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {facebook_G.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(dict(facebook_G.degree()).values()) / facebook_G.number_of_nodes()}\")\n",
    "\n",
    "twitter_G = nx.from_pandas_edgelist(twitter_df, \"start_node\", \"end_node\")\n",
    "print(\"\\n-- Twitter Graph --\")\n",
    "print(f\"Graph type: {type(twitter_G)}\")\n",
    "print(f\"Number of nodes: {twitter_G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {twitter_G.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(dict(twitter_G.degree()).values()) / twitter_G.number_of_nodes()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_sample(G, target_size, seed=42):\n",
    "    random.seed(seed)\n",
    "    start_node = random.choice(list(G.nodes))\n",
    "    visited = {start_node}\n",
    "    current = start_node\n",
    "    neighbors = []\n",
    "\n",
    "    while len(visited) < target_size:\n",
    "        neighbors = [n for n in  neighbors + list(G.neighbors(current)) if n not in visited]\n",
    "        neighbors = list(set(neighbors))\n",
    "        if not neighbors:\n",
    "            start_node = random.choice(list(G.nodes))\n",
    "            visited = {start_node}\n",
    "            current = start_node\n",
    "            continue\n",
    "        current = random.choice(neighbors)\n",
    "        visited.add(current)\n",
    "    return G.subgraph(visited).copy()\n",
    "\n",
    "twitter_G = random_walk_sample(twitter_G, facebook_G.number_of_nodes(), seed=42)\n",
    "print(f\"Number of nodes: {twitter_G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {twitter_G.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(dict(twitter_G.degree()).values()) / twitter_G.number_of_nodes()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have a look at our of the networks using spring layout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(facebook_G, iterations=17, seed=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.axis(\"off\")\n",
    "nx.draw_networkx(facebook_G, pos=pos, ax=ax, node_size=10, with_labels=False, width=0.15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(twitter_G, iterations=17, seed=42) # Try also Twitter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.axis(\"off\")\n",
    "nx.draw_networkx(twitter_G, pos=pos, ax=ax, node_size=10, with_labels=False, width=0.15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Six handshakes\n",
    "\n",
    "<img src=\"https://people.com/thmb/Q_qTsGov1TBp3lCJr0lI0sxP6VA=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(958x681:960x683)/Footloose-Kevin-Bacon-061624-01-20ddcdcf0d8b42f89e9ca908cc63edb1.jpg\" align=\"right\" width=\"20%\"/>\n",
    "\n",
    "There is a famous idea called [**six handshakes rule**](https://en.wikipedia.org/wiki/Six_degrees_of_separation).\n",
    "\n",
    "**Six degrees of separation** is the idea that all people are six or fewer social connections away from each other. </br>\n",
    "As a result, a chain of *friend of a friend* statements can connect any two people in a maximum of **six steps**. </br>\n",
    "It is also known as the **six handshakes rule**.\n",
    "\n",
    "> Compute the average distance between pairs of nodes in the graph using [NetworkX shortest path functions](https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3) Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Communities** in networks are **groups of nodes** that are more **densely connected** to each other than to the rest of the network. \n",
    "- The **Girvan-Newman** algorithm is a method for identifying communities within networks by **iteratively removing edges with high betweenness centrality**.\n",
    "- By targeting and eliminating these critical connections—often the bridges between clusters—the algorithm gradually reveals the network’s underlying community structure, offering valuable insights into how nodes group together.\n",
    "- **Betweenness centrality** measures **how often a node appears on the shortest paths between other nodes in the network**. It is calculated as:\n",
    "\n",
    "$$C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}},$$\n",
    "\n",
    "where $\\sigma_{st}$ is the total number of shortest paths from node $s$ to node $t$, and $\\sigma_{st}(v)$ is the number of those paths that pass through $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(G, edges_to_highlight, colour_map=None, iter_step=-1, func=nx.shell_layout):\n",
    "    try:\n",
    "        layout = func(G, seed=42)\n",
    "    except:\n",
    "        layout = func(G)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    if colour_map is not None:\n",
    "        nx.draw_networkx(G, node_color=colour_map, pos=layout, node_size=40, with_labels=False, width=0.15)\n",
    "        nx.draw_networkx_edges(G, pos=layout, edgelist=edges_to_highlight, width=2, edge_color=(1, 0, 0, 1))\n",
    "    else:\n",
    "        nx.draw_networkx(G, pos=layout, node_size=40, with_labels=False, width=0.15)\n",
    "    ax.axis(\"off\")\n",
    "    if iter_step != -1:\n",
    "        plt.title(f'Step: {iter_step}')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `karate_club_graph` in NetworkX is a well-known social network representing the interactions among members of a karate club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "print(type(G))\n",
    "draw(G, [], iter_step=None, func=nx.spring_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_communities = 3\n",
    "num_conn_comp = 1\n",
    "#draw(G, [], None, 1, func=nx.spring_layout)\n",
    "\n",
    "# Copy of G for modification\n",
    "G_temp = G.copy()\n",
    "\n",
    "# To keep track of removed edges\n",
    "removed_edges = list()\n",
    "\n",
    "# Use a consistent layout for all plots\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# List of distinct colors\n",
    "base_colors = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.BASE_COLORS.values())\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if num_conn_comp == target_communities:\n",
    "        break\n",
    "    \n",
    "    # 1. Compute edge betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G_temp)\n",
    "    \n",
    "    # 2. Remove the edge with highest betweenness\n",
    "    edge_to_remove = max(edge_betweenness, key=edge_betweenness.get)\n",
    "    G_temp.remove_edge(*edge_to_remove)\n",
    "    removed_edges.append(edge_to_remove)\n",
    "    \n",
    "    # 3. Find connected components as communities\n",
    "    communities = list(nx.connected_components(G_temp))\n",
    "    \n",
    "    # 4. Map node to community color\n",
    "    color_map = [''] * G.number_of_nodes()\n",
    "    for color_index, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            color_map[node] = list(mcolors.BASE_COLORS.keys())[color_index % len(mcolors.BASE_COLORS)]\n",
    "            \n",
    "    if len(communities) > num_conn_comp:\n",
    "        #draw(G, removed_edges, color_map, len(communities), func=nx.spring_layout)\n",
    "        num_conn_comp = len(communities)\n",
    "        \n",
    "nx.draw_networkx(G_temp, pos=pos, node_color=color_map, node_size=40, with_labels=False, width=0.15)\n",
    "plt.title(\"Final Community Structure (Between edges ommitted)\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Finding communities using Girvan Newman\n",
    "\n",
    "NetworkX provides an implementation of the Girvan Newman method. </br>\n",
    "> Use NetworkX's `nx.community.girvan_newman` method (find [here](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.centrality.girvan_newman.html)) to regenerate the communities we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 4: Impact of number of communities\n",
    "\n",
    "> Using **Girvan-Newman method**, create loop where you measure the **diameter and an average distance within communites** in a **facebook_G subgraph** (100 nodes) for increasing number of communities (up to $10^{th}$ step).\n",
    "\n",
    "**Hint:** The diameter of a graph is the length of the shortest path between the most distanced nodes (see `nx.diameter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = random_walk_sample(facebook_G, 100, seed=42)\n",
    "\n",
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Centralities: PageRank\n",
    "\n",
    "- PageRank centrality is based on Google's algorithm used to evaluate the importance or relevance of nodes in a network. (See [here](https://en.wikipedia.org/wiki/PageRank))\n",
    "- It assigns **a score to each node** based on the **number and quality of links (or edges) pointing to it**. \n",
    "- It is used to rank web pages in search engines, but it can also be applied to other types of networks to identify key players or important nodes.\n",
    "- It is defined by the **recursive** formula:\n",
    "\n",
    "$$\n",
    "PR(i) = \\frac{1-d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)},\n",
    "$$\n",
    "\n",
    "- where:\n",
    "    - $PR(i)$ is the PageRank of node $i$, \n",
    "    - $d$ is the damping factor (typically set to 0.85), \n",
    "    - $N$ is the total number of nodes, \n",
    "    - $M(i)$ is the set of nodes linking to $i$, and \n",
    "    - $L(j)$ is the number of outbound links from node $j$.\n",
    "- Breaking it down:\n",
    "    - $\\frac{1-d}{N}$ assigns a baseline score to every node, \n",
    "    - The damping factor $d$ represents the probability that **a random surfer continues following links rather than jumping to a random node**,\n",
    "    - The sum $\\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}$ aggregates the contribution of each node $j$ that points to node $i$.\n",
    "- Intuition:\n",
    "    - This formulation captures that **a node’s importance depends not just on the number of incoming links, but on the quality of those links**.\n",
    "    - A node gains more importance if it is **linked by other important nodes that do not spread their influence** too thinly among many targets.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/PageRanks-Example.svg/1920px-PageRanks-Example.svg.png\" alt=\"Source: https://en.wikipedia.org/wiki/PageRank\" width=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "pagerank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "pos = nx.spring_layout(G, seed=42, k=2)\n",
    "\n",
    "\n",
    "node_sizes = [10000 * pagerank[node] for node in G.nodes()]\n",
    "node_colors = [pagerank[node] for node in G.nodes()]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                               node_color=node_colors, cmap=plt.cm.magma)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5, arrows=True, width=2, arrowsize=20, arrowstyle='-|>')\n",
    "plt.colorbar(nodes, label='PageRank Centrality')\n",
    "\n",
    "plt.title(\"Graph Visualization Based on PageRank Centrality\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 5: Impact of number of the damping factor\n",
    "\n",
    "*The damping factor is a parameter that represents the probability that a user will follow a link on a page rather than randomly jumping to any other page.*\n",
    "\n",
    "> Investigate the influence of the damping factor in social media networks. With decreasingly probability of random jumps, what kind of changes do we see in PageRank centralities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by recreating our graphs, this time as ! directed graphs !\n",
    "\n",
    "facebook_G = nx.from_pandas_edgelist(facebook_df, \"start_node\", \"end_node\", create_using=nx.DiGraph())\n",
    "twitter_G = nx.from_pandas_edgelist(twitter_df, \"start_node\", \"end_node\", create_using=nx.DiGraph())\n",
    "#twitter_G = random_walk_sample(twitter_G, facebook_G.number_of_nodes(), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 6: Network communities and local hubs\n",
    "\n",
    "> Examine how the PageRank centralities evolve within the community subgraphs of a Twitter network as the number of communities increases. (up to 4 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
