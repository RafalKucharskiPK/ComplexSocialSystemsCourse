{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13IIprgI-IFj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from pettingzoo.mpe import simple_spread_v3\n",
        "\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KaV8eUE9wpj"
      },
      "outputs": [],
      "source": [
        "EXP_SEED = 42\n",
        "def reset_seeds():\n",
        "    np.random.seed(EXP_SEED)\n",
        "    random.seed(EXP_SEED)\n",
        "reset_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIyr2CbYf83"
      },
      "source": [
        "# 1) Trap Environment and Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99eBj2cLXioK"
      },
      "outputs": [],
      "source": [
        "def show_results(agent, agent_desc, rewards, num_steps, epsilons=None, print_q_table=True):\n",
        "\n",
        "  print()\n",
        "\n",
        "  if print_q_table:\n",
        "    if hasattr(agent, 'q_table'):\n",
        "      print(\"Agent Q-Table:\")\n",
        "      table = agent.q_table\n",
        "      for row_idx, row in enumerate(table):\n",
        "        for col_idx, value in enumerate(row):\n",
        "          print(f\"State {row_idx}, Action {col_idx} : {value:.2f}\")\n",
        "      print()\n",
        "\n",
        "  fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "  axs = axs.flatten()\n",
        "\n",
        "  axs[0].plot(rewards)\n",
        "  axs[0].set_xlabel(\"Episode\")\n",
        "  axs[0].set_ylabel(\"Total Reward\")\n",
        "  axs[0].set_title(\"Episode Reward\")\n",
        "  axs[0].grid(True)\n",
        "\n",
        "  axs[1].plot(num_steps)\n",
        "  axs[1].set_xlabel(\"Episode\")\n",
        "  axs[1].set_ylabel(\"Steps\")\n",
        "  axs[1].set_title(\"Episode Steps\")\n",
        "  axs[1].grid(True)\n",
        "\n",
        "  if epsilons is not None:\n",
        "    axs[2].plot(epsilons)\n",
        "    axs[2].set_xlabel(\"Episode\")\n",
        "    axs[2].set_ylabel(\"Epsilon\")\n",
        "    axs[2].set_title(\"Epsilon Decay\")\n",
        "    axs[2].grid(True)\n",
        "  else:\n",
        "    axs[2].axis('off')\n",
        "\n",
        "  fig.suptitle(f\"{agent_desc}\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSoUfXP1Xod_"
      },
      "outputs": [],
      "source": [
        "def train_agent(agent, env, episodes=500):\n",
        "  rewards, num_steps, epsilons = list(), list(), list()\n",
        "  step_counter = 0\n",
        "\n",
        "  for ep in range(episodes):\n",
        "    print(f\"\\r{ep+1}/{episodes} (Steps={step_counter})\", end=\"\")\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    step_counter = 0\n",
        "    terminated, truncated = False, False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "      step_counter += 1\n",
        "      action = agent.act(state)\n",
        "      next_state, reward, terminated, truncated, info = env.step(action)\n",
        "      agent.learn(state, action, reward, next_state)\n",
        "      state = next_state\n",
        "      total_reward += reward\n",
        "\n",
        "    epsilons.append(getattr(agent, 'epsilon', None))\n",
        "    num_steps.append(step_counter)\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "  return rewards, num_steps, epsilons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3osSxVEJ-EzD"
      },
      "source": [
        "## 1.1) Trap Corridor Environment\n",
        "\n",
        "- A tunnel-shaped environment where agent walks through different cells, trying to reach to the goal cell.\n",
        "- Reaching to goal state results in big reward, while every other movement is penalized with small negative rewards.\n",
        "- Environment includes one trap cell aling the way, which produces small positive rewards for staying in it and a negative reward for leaving it towards the goal.\n",
        "\n",
        "### **Layout**\n",
        "\n",
        "``[ S0 ] -- [ S1 ] -- [S2] -- [S3]``\n",
        "\n",
        "where:\n",
        "- **S0**: Start state\n",
        "- **S1**: **Trap** — leads to small positive reward before goal\n",
        "- **S2**: Intermediate state\n",
        "- **S3**: Goal — **terminal** state\n",
        "\n",
        "### **Actions**\n",
        "- `0 = Stay in place`\n",
        "- `1 = Move right`\n",
        "- `2 = Move left`\n",
        "\n",
        "### **Transitions**\n",
        "\n",
        "|State / Action |   0     |   1     |   2     |\n",
        "|---------------|:-------:|:-------:|:-------:|\n",
        "| S0            |   S0    |   S1    |   S0    |\n",
        "| S1            |   S1    |   S2    |   S0    |\n",
        "| S2            |   S2    |   S3    |   S1    |\n",
        "| S3            |   Terminal                  |\n",
        "\n",
        "### **Rewards**\n",
        "\n",
        "All transitions are penalized with a negative reward -1, except:\n",
        "- Staying in trap state rewarded +1,\n",
        "- Going forward from reward state penalized -10,\n",
        "- Going to terminal state rewarded +100.\n",
        "\n",
        "|State / Action |     0   |     1   |     2   |\n",
        "|---------------|:-------:|:-------:|:-------:|\n",
        "| S0            |   -1    |   -1    |   -1    |\n",
        "| S1            |   +1    |   -10   |   -1    |\n",
        "| S2            |   -1    |   100   |   -1    |\n",
        "| S3            |          Terminal           |\n",
        "\n",
        "### **Termination**\n",
        "- When reached to **S3** OR,\n",
        "- Maximum 10 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldVAYum761lO"
      },
      "outputs": [],
      "source": [
        "class TrapEnv:\n",
        "    def __init__(self):\n",
        "        self.state = 0  # Start at S\n",
        "        self.terminal_state = 3\n",
        "        self.num_states = 4\n",
        "        self.num_actions = 3\n",
        "        self.num_steps = 0\n",
        "        self.max_steps = 10\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self.num_steps = 0\n",
        "        self.state = 0\n",
        "        info = None\n",
        "        return self.state, info\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.num_steps += 1\n",
        "\n",
        "        if action == 0:\n",
        "          next_state = self.state\n",
        "        elif action == 1:\n",
        "          next_state = self.state+1 if self.state < 3 else self.state\n",
        "        elif action == 2:\n",
        "          next_state = self.state-1 if self.state > 0 else self.state\n",
        "        else:\n",
        "          raise ValueError(\"Invalid action\")\n",
        "\n",
        "        if self.state == 1 and next_state == 1:\n",
        "          reward = +1\n",
        "        elif self.state == 1 and next_state == 2:\n",
        "          reward = -10\n",
        "        elif self.state == 2 and next_state == 3:\n",
        "          reward = 100\n",
        "        else:\n",
        "          reward = -1\n",
        "\n",
        "        terminated = next_state == self.terminal_state\n",
        "        truncated = self.num_steps >= self.max_steps\n",
        "        self.state = next_state\n",
        "        \n",
        "        info = None\n",
        "        return next_state, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD5z43eLSiKd"
      },
      "source": [
        "## 1.2) Agents\n",
        "\n",
        "> Let's benchmark three different agents in this environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOTesKSEY0En"
      },
      "source": [
        "### 1.2.1) Random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NIKY8LpW_EY"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "  \"\"\"\n",
        "  An agent that takes random actions.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_actions):\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "  def act(self, state):\n",
        "    return random.randint(0, self.n_actions - 1)\n",
        "\n",
        "  def learn(self, state, action, reward, next_state):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "W_na0nXPXlIG",
        "outputId": "59a23078-2afe-404b-b6a4-c3de7560b1a7"
      },
      "outputs": [],
      "source": [
        "env = TrapEnv()\n",
        "agent = RandomAgent(n_actions=env.num_actions)\n",
        "\n",
        "rewards, num_steps, _ = train_agent(agent, env, episodes=500)\n",
        "show_results(agent, \"Random Agent\", rewards, num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRl1PJUi-TQ6"
      },
      "source": [
        "### 1.2.2) Q-Learning\n",
        "\n",
        "**Q-learning** is an off-policy reinforcement learning algorithm that learns the **optimal action-value function**:\n",
        "\n",
        "Q(s, a) represents the expected cumulative reward by taking action $a$ in state $s$ and following the best policy after that.\n",
        "\n",
        "> Update Rule\n",
        "\n",
        "Q-value expectation are updated using the **Bellman Optimality Equiation**.\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\alpha $: learning rate  \n",
        "- $ \\gamma $: discount factor  \n",
        "- $ r $: immediate reward after taking action  \n",
        "- $ s' $: next state  \n",
        "- $ \\max_{a'} Q(s', a') $: estimate of future rewards\n",
        "\n",
        "---\n",
        "\n",
        "> Greedy policy\n",
        "\n",
        "$$\n",
        "a_{t} = \\arg\\max_a Q(s, a)\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_ESZxk7Wz5c"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent(RandomAgent):\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99):\n",
        "        super().__init__(n_actions)\n",
        "        self.q_table = np.zeros((n_states, n_actions))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def act(self, state):\n",
        "        return int(np.argmax(self.q_table[state]))\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        best_next = np.max(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.alpha * td_error\n",
        "\n",
        "    def get_policy(self):\n",
        "        return np.argmax(self.q_table, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "7sPZabiyZQgL",
        "outputId": "20a2f70b-46fb-4b3b-c3a5-9231834a7e53"
      },
      "outputs": [],
      "source": [
        "env = TrapEnv()\n",
        "agent = QLearningAgent(n_states=env.num_states, n_actions=env.num_actions)\n",
        "\n",
        "rewards, num_steps, _ = train_agent(agent, env, episodes=500)\n",
        "show_results(agent, \"Q Learning Agent\", rewards, num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjOR4PtzUjV1"
      },
      "source": [
        "### 1.2.3) Epsilon-Greedy Policy\n",
        "\n",
        "An agent needs balance between:\n",
        "- Exploring unknown solutions.\n",
        "- Exploiting previously explored solutions.\n",
        "\n",
        "For a sufficient exploration, one of the most used techniques (particularly in value-based methods) is called **Epsilon-greeedy**.\n",
        "\n",
        "By this, agent makes random decisions for explorations, and slowly becoming deterministic in the later stages of the training.\n",
        "\n",
        "This procedure can be handled with two parameters:\n",
        "- $\\epsilon$: A parameter determining the randomness of the policy.\n",
        "- $\\epsilon$ decay rate: A parameter determining how fast the agent will become deterministic.\n",
        "\n",
        "$$\n",
        "a_t =\n",
        "\\begin{cases}\n",
        "\\text{random action} & \\text{with probability } \\epsilon, \\\\\\\\\n",
        "\\underset{a}{\\arg\\max}\\, Q(s_t, a) & \\text{with probability } 1 - \\epsilon.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where, $\\epsilon$ is decayed iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EtduDDh64r8"
      },
      "outputs": [],
      "source": [
        "class QLearningEpsGreedyAgent(QLearningAgent):\n",
        "    def __init__(self, n_states, n_actions, alpha=0.2, gamma=0.9, epsilon=0.99, eps_decay_rate=0.99):\n",
        "        super().__init__(n_states, n_actions, alpha, gamma)\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay_rate = eps_decay_rate\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        super().learn(state, action, reward, next_state)\n",
        "        self.epsilon *= self.eps_decay_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "K6rS4cim7Iwv",
        "outputId": "5c84d7b6-c40d-46ac-819e-ed279818219a"
      },
      "outputs": [],
      "source": [
        "env = TrapEnv()\n",
        "agent = QLearningEpsGreedyAgent(n_states=env.num_states, n_actions=env.num_actions)\n",
        "\n",
        "rewards, num_steps, epsilons = train_agent(agent, env, episodes=500)\n",
        "show_results(agent, \"Q Learning Epsilon Greedy Agent\", rewards, num_steps, epsilons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pym0fxzIa_As"
      },
      "source": [
        "# Gym\n",
        "\n",
        "[Gym](https://www.gymlibrary.dev/index.html) is a Python toolkit for developing and comparing reinforcement learning algorithms.\n",
        "\n",
        "It provides:\n",
        "- A wide range of standardized environments.\n",
        "- A simple API: `env.reset()`, `env.step(action)`, `env.render()`\n",
        "- Easy integration with RL libraries like Stable-Baselines3, RLlib, and others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Frozen Lake\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" width=\"200\"/>\n",
        "</p>\n",
        "\n",
        "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake. [Details](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False)\n",
        "env.action_space.seed(EXP_SEED)\n",
        "agent = QLearningEpsGreedyAgent(env.observation_space.n,\n",
        "                                env.action_space.n,\n",
        "                                epsilon=1.0,\n",
        "                                eps_decay_rate=0.9999)\n",
        "\n",
        "rewards, num_steps, epsilons = train_agent(agent, env, episodes=2000)\n",
        "env.close()\n",
        "show_results(agent, \"Q Learning Epsilon Greedy Agent\", rewards, num_steps, epsilons, print_q_table=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "num_test_episodes = 10\n",
        "env = gym.make('FrozenLake-v1', render_mode='human', map_name=\"4x4\", is_slippery=False)\n",
        "\n",
        "for _ in range(num_test_episodes):\n",
        "  state, _ = env.reset()\n",
        "  terminated, truncated = False, False\n",
        "  step_counter = 0\n",
        "  while not (terminated or truncated):\n",
        "    step_counter += 1\n",
        "    action = np.argmax(agent.q_table[state])\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    time.sleep(0.2)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "Xxoz4Ry5d3vL",
        "outputId": "91657a9c-a44b-4e75-ccd4-4cbba3769573"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False)\n",
        "env.action_space.seed(EXP_SEED)\n",
        "agent = QLearningEpsGreedyAgent(env.observation_space.n,\n",
        "                                env.action_space.n,\n",
        "                                epsilon=1.0,\n",
        "                                eps_decay_rate=0.999999)\n",
        "\n",
        "rewards, num_steps, epsilons = train_agent(agent, env, episodes=16000)\n",
        "env.close()\n",
        "show_results(agent, \"Q Learning Epsilon Greedy Agent\", rewards, num_steps, epsilons, print_q_table=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Fp_XvClidcPB",
        "outputId": "c4a572ba-b8ea-407c-8092-2db42349230b"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "num_test_episodes = 10\n",
        "env = gym.make('FrozenLake-v1', render_mode='human', map_name=\"8x8\", is_slippery=False)\n",
        "\n",
        "for _ in range(num_test_episodes):\n",
        "  state, _ = env.reset()\n",
        "  terminated, truncated = False, False\n",
        "  step_counter = 0\n",
        "  while not (terminated or truncated):\n",
        "    step_counter += 1\n",
        "    action = np.argmax(agent.q_table[state])\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    time.sleep(0.2)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Q-Network (DQN)\n",
        "\n",
        "DQN is a **value-based** RL algorithm that **approximates** the Q-function using a neural network.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2023/04/dql-vs-ql-1.png\" width=\"600\"/>\n",
        "</p>\n",
        "\n",
        "The version we will see today (Double DQN) uses:\n",
        "- An policy network $Q_{\\text{policy}}$ to learn the Q-values\n",
        "- A target network $Q_{\\text{target}}$ to compute stable targets\n",
        "- A replay buffer to store and sample past experiences\n",
        "\n",
        "### Update Rule\n",
        "\n",
        "Given a transition $(s, a, r, s{\\prime}, \\text{done})$, the target is:\n",
        "$$\n",
        "y = r + \\gamma \\cdot \\max_{a{\\prime}} Q_{\\text{target}}(s{\\prime}, a{\\prime}) \\cdot (1 - \\text{done})\n",
        "$$\n",
        "The loss minimized is:\n",
        "$$\n",
        "L = \\left( Q_{\\text{policy}}(s, a) - y \\right)^2\n",
        "$$\n",
        "\n",
        "Workflow:\n",
        "1.\tStore each experience in the replay buffer\n",
        "2.\tSample a batch of transitions from the buffer\n",
        "3.\tCompute target values using the target network\n",
        "4.\tUpdate the policy network by minimizing the loss\n",
        "5.\tPeriodically copy policy weights to the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment config\n",
        "NUM_AGENTS = 3\n",
        "MAX_CYCLES = 10\n",
        "NUM_EPISODES = 1500\n",
        "\n",
        "# DQN Parameters\n",
        "BUFFER_SIZE = 100000\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 10000\n",
        "LR = 0.0005\n",
        "TARGET_UPDATE_FREQ = 20\n",
        "HIDDEN_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=HIDDEN_SIZE):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "    \n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'reward', 'next_state', 'done'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        actual_batch_size = min(batch_size, len(self.memory))\n",
        "        return random.sample(self.memory, actual_batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, agent_id, state_size, action_size, device):\n",
        "        self.agent_id_str = agent_id\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = device\n",
        "\n",
        "        # Q-Network & Target Network\n",
        "        self.policy_net = QNetwork(state_size, action_size).to(device)\n",
        "        self.target_net = QNetwork(state_size, action_size).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "        # Each agent has its own buffer\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE)\n",
        "\n",
        "        # Epsilon-greedy parameters\n",
        "        self.epsilon = EPS_START\n",
        "        self.steps_done = 0 # use for decay\n",
        "\n",
        "    def act(self, state):\n",
        "        # Choose an action using epsilon-greedy policy.\n",
        "        decay_steps = EPS_DECAY\n",
        "        self.epsilon = max(EPS_END, EPS_START - (EPS_START - EPS_END) * (self.steps_done / decay_steps))\n",
        "        # Note: steps_done is incremented globally in the training loop\n",
        "\n",
        "        if random.random() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                if not isinstance(state, np.ndarray):\n",
        "                     state = np.array(state, dtype=np.float32)\n",
        "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "                action_values = self.policy_net(state_tensor)\n",
        "                action = np.argmax(action_values.cpu().data.numpy())\n",
        "                return action\n",
        "        else:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "    def learn(self):\n",
        "        #Update policy network using samples from replay buffer\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return None # Not enough samples yet\n",
        "\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # Convert batch arrays to tensors\n",
        "        state_batch = torch.tensor(np.array(batch.state), dtype=torch.float32, device=self.device)\n",
        "        action_batch = torch.tensor(batch.action, dtype=torch.long, device=self.device).unsqueeze(1)\n",
        "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        next_state_batch = torch.tensor(np.array(batch.next_state), dtype=torch.float32, device=self.device)\n",
        "        done_batch = torch.tensor(batch.done, dtype=torch.float32, device=self.device).unsqueeze(1) # Boolean/int to float\n",
        "\n",
        "        # Compute Q(s_t, a) from POLICY network\n",
        "        q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute V(s_{t+1}) using the TARGET network\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # Compute the expected Q values: R + gamma * max_a' Q_target(s', a') * (1 - done)\n",
        "        expected_q_values = reward_batch + (GAMMA * next_q_values * (1 - done_batch))\n",
        "\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(q_values, expected_q_values)\n",
        "\n",
        "        # Training\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_net(self):\n",
        "        # Copy weights from policy network to target network\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PettingZoo\n",
        "\n",
        "PettingZoo is a Python library for MARL environments, similar to how OpenAI Gym is used for single-agent RL.\n",
        "\n",
        "It provides:\n",
        "- A standard API for turn-based (AEC), simultaneous (parallel), and mixed environments\n",
        "- A wide range of environments: from classic games to robotics and ecology\n",
        "- Easy integration with RL libraries like Stable-Baselines3 and RLlib\n",
        "\n",
        "Details [here](https://pettingzoo.farama.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Spread\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://pettingzoo.farama.org/_images/mpe_simple_spread.gif\" width=\"200\"/>\n",
        "</p>\n",
        "\n",
        "Agents must learn to cover all the landmarks while avoiding collisions.\n",
        "\n",
        "- All agents are globally rewarded based on how far the closest agent is to each landmark (sum of the minimum distances).\n",
        "- Locally, the agents are penalized if they collide with other agents (-1 for each collision). \n",
        "- The relative weights of these rewards can be controlled with the local_ratio parameter (as we do below).\n",
        "- Agent observations: `[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, communication]`\n",
        "- Agent action space: `[no_action, move_left, move_right, move_down, move_up]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env):\n",
        "    print(\"Starting Training with PettingZoo simple_spread_v3...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    agent_ids = env.possible_agents\n",
        "    sample_agent_id = agent_ids[0]\n",
        "    observation_space = env.observation_space(sample_agent_id)\n",
        "    action_space = env.action_space(sample_agent_id)\n",
        "    print(f\"Agent IDs: {agent_ids}\")\n",
        "\n",
        "    state_size = observation_space.shape[0]\n",
        "    action_size = action_space.n\n",
        "    print(f\"State size: {state_size}, Action size: {action_size}\")\n",
        "\n",
        "    # dict of agents, PettingZoo ID : agent object\n",
        "    agents = {agent_id: DQNAgent(agent_id, state_size, action_size, device)\n",
        "              for agent_id in agent_ids}\n",
        "\n",
        "    episode_rewards_history = []\n",
        "    total_steps_global = 0\n",
        "    \n",
        "    for i_episode in range(1, NUM_EPISODES + 1):\n",
        "        observations, infos = env.reset()\n",
        "        current_episode_reward = 0\n",
        "        episode_loss = 0.0\n",
        "        steps_in_episode = 0\n",
        "\n",
        "        for _ in range(MAX_CYCLES):\n",
        "            total_steps_global += 1\n",
        "            steps_in_episode += 1\n",
        "\n",
        "            # For epsilon decay\n",
        "            for agent in agents.values():\n",
        "                 agent.steps_done = total_steps_global\n",
        "\n",
        "            # Collect actions from all agents\n",
        "            actions = {}\n",
        "            for agent_id in env.agents: # env.agents lists currently active agents!1\n",
        "                if agent_id in observations:\n",
        "                    obs = observations[agent_id]\n",
        "                    actions[agent_id] = agents[agent_id].act(obs)\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
        "\n",
        "            # Finished agents\n",
        "            dones = {agent_id: terminations.get(agent_id, False) or truncations.get(agent_id, False) for agent_id in agents.keys()}\n",
        "\n",
        "            # Store experience and learn for each agent that took an action\n",
        "            loss_val_step = 0\n",
        "            num_active_agents = 0\n",
        "            active_agents_before_step = list(actions.keys())\n",
        "            for agent_id in active_agents_before_step:\n",
        "                # Ensure all necessary data exists\n",
        "                if agent_id in observations and agent_id in actions and agent_id in rewards and agent_id in next_observations and agent_id in dones:\n",
        "                    state = observations[agent_id]\n",
        "                    action = actions[agent_id]\n",
        "                    reward = sum(rewards.values()) / NUM_AGENTS\n",
        "                    # We are using average global reward, doesn't have to be the case\n",
        "                    next_state = next_observations[agent_id]\n",
        "                    done = dones[agent_id]\n",
        "\n",
        "                    # Push to specific agent's replay buffer\n",
        "                    agents[agent_id].memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "                    # Perform learning step for the agent\n",
        "                    loss = agents[agent_id].learn()\n",
        "                    if loss is not None:\n",
        "                        loss_val_step += loss\n",
        "                        num_active_agents += 1\n",
        "\n",
        "            observations = next_observations\n",
        "            current_episode_reward += sum(rewards.values())\n",
        "            if num_active_agents > 0:\n",
        "                episode_loss += loss_val_step / num_active_agents\n",
        "\n",
        "            if not env.agents: # iF no agents left in the environment\n",
        "                 break\n",
        "\n",
        "        # --- End of Episode ---\n",
        "        episode_rewards_history.append(current_episode_reward)\n",
        "        avg_loss = episode_loss / steps_in_episode if steps_in_episode > 0 else 0\n",
        "\n",
        "        # Update target network periodically (parameterized above)\n",
        "        if i_episode % TARGET_UPDATE_FREQ == 0:\n",
        "            for agent in agents.values():\n",
        "                agent.update_target_net()\n",
        "\n",
        "        # Print progress\n",
        "        if i_episode % 20 == 0:\n",
        "             avg_reward = np.mean(episode_rewards_history[-20:])\n",
        "             current_epsilon = agents[sample_agent_id].epsilon\n",
        "             print(f\"\\rEp {i_episode}/{NUM_EPISODES} | Avg Reward (last 20): {avg_reward:.2f} | Last Reward: {current_episode_reward:.2f} | Avg Loss: {avg_loss:.4f} | Epsilon: {current_epsilon:.3f} | Steps: {total_steps_global}\", end=\"\")\n",
        "\n",
        "    env.close()\n",
        "    print(\"\\n\\nTraining finished.\")\n",
        "    return agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = simple_spread_v3.parallel_env(N=NUM_AGENTS, local_ratio=0.5,\n",
        "                                    max_cycles=MAX_CYCLES,\n",
        "                                    continuous_actions=False)\n",
        "    \n",
        "trained_agents_dict = train(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def showcase(trained_agents, num_episodes=3):\n",
        "    print(\"\\n--- Showcasing Trained Policy ---\")\n",
        "    if not trained_agents:\n",
        "        print(\"No trained agents provided.\")\n",
        "        return\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    try:\n",
        "        env = simple_spread_v3.parallel_env(N=NUM_AGENTS, local_ratio=0.5,\n",
        "                                            max_cycles=MAX_CYCLES,\n",
        "                                            continuous_actions=False,\n",
        "                                            render_mode=\"human\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating environment for rendering (ensure pygame is installed): {e}\")\n",
        "        print(\"Skipping showcase.\")\n",
        "        return\n",
        "\n",
        "    agents_to_showcase = trained_agents\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        print(f\"\\nShowcase Episode {i_episode + 1}\")\n",
        "        observations, infos = env.reset()\n",
        "        total_reward_episode = 0\n",
        "\n",
        "        for step in range(MAX_CYCLES):\n",
        "            env.render()\n",
        "            time.sleep(0.05)\n",
        "\n",
        "            actions = {}\n",
        "            current_agents = env.agents # active agents for this step\n",
        "            if not current_agents:\n",
        "                break\n",
        "\n",
        "            for agent_id in current_agents:\n",
        "                 if agent_id in observations:\n",
        "                     state = observations[agent_id]\n",
        "                     # Choose action GREEDILY\n",
        "                     with torch.no_grad():\n",
        "                         if not isinstance(state, np.ndarray):\n",
        "                             state = np.array(state, dtype=np.float32)\n",
        "                         state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "                         # Use the policy_nets\n",
        "                         action_values = agents_to_showcase[agent_id].policy_net(state_tensor)\n",
        "                         action = np.argmax(action_values.cpu().data.numpy())\n",
        "                         actions[agent_id] = action\n",
        "\n",
        "\n",
        "            if actions:\n",
        "                 next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
        "                 observations = next_observations\n",
        "                 step_reward = sum(rewards.values())\n",
        "                 total_reward_episode += step_reward\n",
        "            else:\n",
        "                 break\n",
        "\n",
        "\n",
        "        print(f\"End of Showcase Episode {i_episode + 1}. Total Reward: {total_reward_episode:.2f}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "showcase(trained_agents_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
